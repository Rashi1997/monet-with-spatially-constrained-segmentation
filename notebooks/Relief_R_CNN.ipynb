{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relief R-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1re_IaIQM76riKWSbnLhC2juBdtdj842r",
      "authorship_tag": "ABX9TyO3U60qMv2smAkTRotALG2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmoopenn/unsupervised-object-detection-with-rpn/blob/master/Relief_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_hHmT1ILtWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "device = torch.device(dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vdzi4m4joyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clone repo\n",
        "!git clone https://github.com/mila-iqia/atari-representation-learning.git\n",
        "\n",
        "# PyTorch and scikit learn\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Baselines for Atari preprocessing\n",
        "# Tensorflow is a dependency, but you don't need to install the GPU version\n",
        "!pip install git+git://github.com/openai/baselines\n",
        "\n",
        "# pytorch-a2c-ppo-acktr for RL utils\n",
        "!pip install git+git://github.com/ankeshanand/pytorch-a2c-ppo-acktr-gail\n",
        "\n",
        "# Clone and install package\n",
        "!pip install -r atari-representation-learning/requirements.txt\n",
        "!pip install git+git://github.com/mila-iqia/atari-representation-learning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYzcvmSKpjPL",
        "colab_type": "text"
      },
      "source": [
        "Build a standard AutoEncoder that will take in an atari frame as input and its output will be a reconstruction of the input. In the training process, we are interested in the filters that the network learns more than the latent space representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Mvrna6p8Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "## AutoEncoder Definition \n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encoder = nn.Sequential(OrderedDict([ \n",
        "        ('conv1', nn.Conv2d(1, 16, 3, stride=2, padding=1)),\n",
        "        ('relu1', nn.ReLU()),\n",
        "        ('conv2', nn.Conv2d(16, 32, 3, stride=2, padding=1)),\n",
        "        ('relu2', nn.ReLU()),\n",
        "        ('conv3', nn.Conv2d(32, 64, 7))\n",
        "    ]))\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(64, 32, 7),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.encoder(x)\n",
        "    x_hat = self.decoder(z)\n",
        "    return x_hat\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2YEvUIw1006",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example_model = AutoEncoder()\n",
        "\n",
        "## PRINT LAYERS ##\n",
        "# for k,v in example_model.state_dict().items():\n",
        "#   print(\"Layer {}\".format(k))\n",
        "#   print(v)\n",
        "\n",
        "## Get a iterator for layers of the encoder ##\n",
        "## tuples of the form (layer name, layer) ##\n",
        "encoder_layers = [layer for layer in example_model.encoder.named_children()]\n",
        "e1_name, e1_layer = encoder_layers[0]\n",
        "\n",
        "## Layer name ## \n",
        "print(e1_name)\n",
        "\n",
        "## Layer weights ## \n",
        "print(e1_layer.weight)\n",
        "\n",
        "## Layer bias ## \n",
        "print(e1_layer.bias)\n",
        "\n",
        "## Check Layer's type ##\n",
        "print(isinstance(e1_layer, nn.Conv2d)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYbcBTdsYV9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGHQoMA-sIbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "from os import sys \n",
        "\n",
        "#sys.path.append('/content/drive/My\\ Drive/atari-representation-learning')\n",
        "#%cd drive/My\\ Drive/atari-representation-learning\n",
        "from atariari.benchmark.episodes import get_ppo_rollouts, get_random_agent_rollouts \n",
        "\n",
        "\n",
        "def get_episodes(env_name,\n",
        "                 steps,\n",
        "                 seed=42,\n",
        "                 num_processes=1,\n",
        "                 num_frame_stack=1,\n",
        "                 downsample=False,\n",
        "                 color=False,\n",
        "                 entropy_threshold=0.6,\n",
        "                 collect_mode=\"random_agent\",\n",
        "                 train_mode=\"train_encoder\",\n",
        "                 checkpoint_index=-1,\n",
        "                 min_episode_length=64):\n",
        "\n",
        "    if collect_mode == \"random_agent\":\n",
        "        # List of episodes. Each episode is a list of 160x210 observations\n",
        "        episodes, episode_labels = get_random_agent_rollouts(env_name=env_name,\n",
        "                                                             steps=steps,\n",
        "                                                             seed=seed,\n",
        "                                                             num_processes=num_processes,\n",
        "                                                             num_frame_stack=num_frame_stack,\n",
        "                                                             downsample=downsample, color=color)\n",
        "\n",
        "    elif collect_mode == \"pretrained_ppo\":\n",
        "        # List of episodes. Each episode is a list of 160x210 observations\n",
        "        episodes, episode_labels = get_ppo_rollouts(env_name=env_name,\n",
        "                                                   steps=steps,\n",
        "                                                   seed=seed,\n",
        "                                                   num_processes=num_processes,\n",
        "                                                   num_frame_stack=num_frame_stack,\n",
        "                                                   downsample=downsample,\n",
        "                                                   color=color,\n",
        "                                                   checkpoint_index=checkpoint_index)\n",
        "\n",
        "\n",
        "    else:\n",
        "      assert False, \"Collect mode {} not recognized\".format(collect_mode)\n",
        "\n",
        "    # Get indices for episodes that have min_episode_length\n",
        "    ep_inds = [i for i in range(len(episodes)) if len(episodes[i]) > min_episode_length]\n",
        "    episodes = [episodes[i] for i in ep_inds]\n",
        "    \n",
        "    # Shuffle\n",
        "    inds = np.arange(len(episodes))\n",
        "    rng = np.random.RandomState(seed=seed)\n",
        "    rng.shuffle(inds)\n",
        "\n",
        "    if train_mode == \"train_encoder\":\n",
        "      assert len(inds) > 1, \"Not enough episodes to split into train and val. You must specify enough steps to get at least two episodes\"\n",
        "      split_ind = int(0.8 * len(inds))\n",
        "      tr_eps, val_eps = episodes[:split_ind], episodes[split_ind:]\n",
        "      return tr_eps, val_eps\n",
        "\n",
        "    if train_mode == \"dry_run\":\n",
        "      return episodes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utIpXGK8it6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  from torch.utils.data import RandomSampler, BatchSampler\n",
        "  import torch.nn.functional as F\n",
        "  \n",
        "  def generate_batch(episodes, batch_size):\n",
        "      total_steps = sum([len(e) for e in episodes])\n",
        "      print('Total Steps: {}'.format(total_steps))\n",
        "      # Episode sampler\n",
        "      # Sample `num_samples` episodes then batchify them with `self.batch_size` episodes per batch\n",
        "      sampler = BatchSampler(RandomSampler(range(len(episodes)),\n",
        "                                            replacement=True, num_samples=total_steps),\n",
        "                              batch_size, drop_last=True)\n",
        "      for indices in sampler:\n",
        "          episodes_batch = [episodes[x] for x in indices]\n",
        "          x_t, x_tprev, x_that, ts, thats = [], [], [], [], []\n",
        "          for episode in episodes_batch:\n",
        "              # Get one sample from this episode\n",
        "              t, t_hat = 0, 0\n",
        "              t, t_hat = np.random.randint(0, len(episode)), np.random.randint(0, len(episode))\n",
        "              frame = episode[t]\n",
        "              resized_frame = F.interpolate(frame.unsqueeze(0) / 255.0, size=160, mode='bicubic').squeeze(0)\n",
        "              x_t.append(resized_frame)\n",
        "          yield torch.stack(x_t).float().to(device) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5HJSdfwrwJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(model, game, total_steps, batch_size=64, learning_rate=1e-3,  train_mode=\"train_encoder\", agent_type=\"random_agent\"):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate, \n",
        "                                 weight_decay=1e-5) # <--\n",
        "    train_eps, validation_eps = get_episodes(game, total_steps,\n",
        "                                             collect_mode=agent_type)\n",
        "    outputs = []\n",
        "    print(\"Num episodes\", len(train_eps))\n",
        "    data_generator = generate_batch(train_eps, batch_size)\n",
        "    for x_batch in data_generator:\n",
        "      #print(\"batch shape\", x_batch.shape)\n",
        "      recon = model(x_batch)\n",
        "      loss = criterion(recon, x_batch)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "\n",
        "      print('Loss:{:.4f}'.format( float(loss)))\n",
        "      outputs.append((x_batch, recon),)\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrvjjZ_T-SOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, game, steps_per_epoch, num_epochs=2, batch_size=64, learning_rate=1e-3):\n",
        "  for ep in range(num_epochs):\n",
        "    if ep % 2 == 0:\n",
        "      train_one_epoch(model, game, steps_per_epoch, agent_type=\"random_agent\")\n",
        "    else:\n",
        "      train_one_epoch(model, game, steps_per_epoch, agent_type=\"pretrained_ppo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXk9JfC0_DDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TRAIN THE MODEL ## \n",
        "model = AutoEncoder()\n",
        "model.to(device)\n",
        "\n",
        "# check if model on gpu\n",
        "print(next(model.parameters()).is_cuda)\n",
        "\n",
        "game = 'PitfallNoFrameskip-v4'\n",
        "train(model, game, 2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCPzOdcwOGnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_maps(model, image, target_layer_name):\n",
        "  encoder_layers = [l for l in model.encoder.named_children()]\n",
        "\n",
        "  feature_maps = []\n",
        "  x = image\n",
        "  \n",
        "  # Gather feature maps for a specified layer of the encoder\n",
        "  for layer_name, layer in encoder_layers:\n",
        "    intermediate = layer(x)\n",
        "    if layer_name == target_layer_name:\n",
        "      feature_maps = intermediate\n",
        "      break\n",
        "    x = intermediate\n",
        "\n",
        "  return feature_maps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0LCuTU6OaFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def integrate_feature_maps(feature_maps):\n",
        "  # Normalize each feature map by its maximal feature value\n",
        "  # (batch_size, channels, h, w) -> (batch_size, channels) \n",
        "  max_pixel_values = np.max(feature_maps, axis=(2,3))\n",
        "  # unsqueeze last dimension twice so we can divide (batch_size, channels) -> (batch_size, channels, 1, 1) \n",
        "  max_pixel_values = np.expand_dims(np.expand_dims(max_pixel_values,-1), -1)\n",
        "  feature_maps = feature_maps / max_pixel_values\n",
        "\n",
        "  # Integrate feature maps by adding normalized feature maps together element-wise \n",
        "  f_integrate = np.sum(feature_maps, 1)\n",
        "\n",
        "  return f_integrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Eytmf2OvtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_feature_levels(f_integrate, num_subranges):\n",
        "  # Define feature level subranges \n",
        "  min_feature_value, max_feature_value = np.min(f_integrate), np.max(f_integrate)\n",
        "  #feature_levels = torch.linspace(min_value, max_value, steps=num_subranges)\n",
        "\n",
        "  stride = (max_feature_value - min_feature_value) / num_subranges\n",
        "  feature_levels = [min_feature_value + stride * i for i in range(num_subranges)]\n",
        "\n",
        "  return stride, feature_levels "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSjlunLPPTre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_level_indices(f_integrate, feature_levels):\n",
        "  # Switch to numpy values since no digitize function in torch \n",
        "  #feature_levels_np, stride_np = feature_levels.numpy(), stride.numpy()\n",
        "  #f_integrate_np = f_integrate\n",
        "  batch_dim = f_integrate.shape[0]\n",
        "\n",
        "  # flatten f_integrate, assign each feature value to a feature level then reshape into 2D\n",
        "  feature_level_indices = np.digitize(f_integrate.reshape(batch_dim, -1), feature_levels).reshape((-1, f_integrate.shape[1], f_integrate.shape[2]))\n",
        "\n",
        "  return feature_level_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y3mgqukQA2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from queue import Queue\n",
        "import itertools \n",
        "\n",
        "### TODO: CHANGE FROM SEARCH STYLE TO CALCULATION STYLE SO IT CAN BE BATCHED AND PARALLELIZED\n",
        "\n",
        "def generate_small_rois(feature_level_indices, target_level):\n",
        "  print(feature_level_indices.shape)\n",
        "  x_indices, y_indices = np.where(feature_level_indices == target_level)\n",
        "  num_candidates = x_indices.shape[0]\n",
        "  visited = set()\n",
        "  max_width, max_height = feature_level_indices.shape[1], feature_level_indices.shape[0]\n",
        "  bboxes = np.zeros((0,4))\n",
        "  while num_candidates > 0:\n",
        "    # Run BFS to find to ROIs \n",
        "    q = Queue()\n",
        "    x, y = list(x_indices).pop(), list(y_indices).pop()\n",
        "    num_candidates -= 1\n",
        "    #print(\"CANDIDATES\", num_candidates)\n",
        "    if (x,y) in visited:\n",
        "      continue\n",
        "    else:\n",
        "      q.put((x, y))\n",
        "      visited.add((x,y))\n",
        "      x_min = x_max = x\n",
        "      y_min = y_max = y\n",
        "    while not q.empty():\n",
        "      xy_coordinate = q.get()\n",
        "      visited.add(xy_coordinate)\n",
        "      x, y = xy_coordinate[0], xy_coordinate[1]\n",
        "      # get cross product of two lists to generate all directions for neighboring pixel cells\n",
        "      deltas = [delta for delta in itertools.product([-1,0,1], [-1,0,1])]\n",
        "      # remove (0,0)\n",
        "      deltas = set(deltas).difference(set([0,0]))\n",
        "      for delta_x, delta_y in deltas:\n",
        "        new_x, new_y = x + delta_x, y + delta_y\n",
        "        # check pixel value is a valid position or visited already\n",
        "        if new_x < 0 or new_y < 0 or new_x >= max_width or new_y >= max_height or (new_x, new_y) in visited:\n",
        "          continue \n",
        "        # update bounding box if pixel value is target_level\n",
        "        if feature_level_indices[new_x][new_y] == target_level:\n",
        "          x_min = min(x_min, new_x)\n",
        "          y_min = min(y_min, new_y)\n",
        "          x_max = max(x_max, new_x)\n",
        "          y_max = max(y_max, new_y)\n",
        "          q.put((new_x, new_y))\n",
        "          visited.add((new_x, new_y))\n",
        "    bboxes = np.vstack((bboxes, np.array([x_min, y_min, x_max, y_max])))\n",
        "  print(\"PROPOSED REGIONS\", bboxes.shape[0])\n",
        "  return bboxes\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvhPsJ51QIZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_one_large_roi(feature_level_indices, target_level):\n",
        "  indices = np.where(feature_level_indices == target_level)\n",
        "  if indices[0].shape[0] == 0:\n",
        "    return np.array([])\n",
        "  x_indices, y_indices = indices[0], indices[1]\n",
        "  return np.array([min(x_indices),\n",
        "                   min(y_indices),\n",
        "                   max(x_indices),\n",
        "                   max(y_indices)\n",
        "                  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThKjx5HhQMF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_rois(feature_level_indices, target_level):\n",
        "  small_ROIs = generate_small_rois(feature_level_indices, target_level)\n",
        "  large_ROI = generate_one_large_roi(feature_level_indices, small_ROIs)\n",
        "  return small_ROIs, large_ROI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z_GRY9WBHJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Relief Region/Object Proposal Algorithm \n",
        "\n",
        "def relief_proposals(model, image, target_layer_name, num_subranges):\n",
        "  ### GET FEATURE MAPS FROM ENCODER ###\n",
        "  feature_maps = get_feature_maps(model, image, target_layer_name)\n",
        "  feature_maps = feature_maps.to(\"cpu\")\n",
        "  feature_maps = feature_maps.detach().numpy()\n",
        "  print(\"FEATURE MAPS SHAPE\", feature_maps.shape)\n",
        "\n",
        "  ### GENERATE INTEGRATED FEATURE MAP ###\n",
        "  f_integrate = integrate_feature_maps(feature_maps)\n",
        "  #print(f_integrate)\n",
        "  print(\"FINTEGRATE SHAPE\", f_integrate.shape)\n",
        "\n",
        "  ### FEATURE LEVEL DEFINITIONS ###\n",
        "  stride, feature_levels = define_feature_levels(f_integrate, num_subranges)\n",
        "  print(\"STRIDE\", stride)\n",
        "  print(feature_levels)\n",
        "\n",
        "  ### FEATURE LEVEL ASSIGNMENTS ###\n",
        "  feature_level_indices = get_feature_level_indices(f_integrate, feature_levels)\n",
        "  #print(feature_level_indices)\n",
        "  print(feature_level_indices.shape)\n",
        "\n",
        "  ### ROI GENERATION ### \n",
        "  feature_level_dictionary = {}\n",
        "  for l in range(num_subranges):\n",
        "    small, large = [], []\n",
        "    print(\"######################## LEVEL {} #####################\".format(l))\n",
        "    for i in range(feature_level_indices.shape[0]):\n",
        "      small_rois, large_roi = generate_rois(feature_level_indices[i], l)\n",
        "      small.append(small_rois)\n",
        "      large.append(large_roi)\n",
        "    feature_level_dictionary[l] = (small, large)\n",
        "  \n",
        "  ### CAN ALSO COMBINE SMALL ROIs into LARGER ROIs GIVEN CERTAIN RULES ###\n",
        "\n",
        "\n",
        "  ### TODO: LOCAL SEARCH TO REFINE ROIs ### \n",
        "\n",
        "\n",
        "  return feature_level_dictionary \n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNkaWbvUGkZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_eps, validation_eps = get_episodes(game, 1000, collect_mode='pretrained_ppo')\n",
        "data_generator = generate_batch(train_eps, 32)\n",
        "images = [x for x in data_generator]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwHek5WdH9wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_level_bboxes = relief_proposals(model, images[0], 'conv1', 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-23ZM4qzLVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}